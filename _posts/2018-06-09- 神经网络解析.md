---
layout: post
title: 神经网络解析
date: 2018-06-08
categories: 深度学习
tags: [神经网络]
---

> 摘要：通过对一个最简单的神经网络模型的研究，理解激励函数，损失函数，梯度下降等概念。

# 一 简介

神经网络是一种对人脑的仿生学实现，目前在某些领域已经有了较好的模拟，但还远远不够。目前随着技术在软件（各种深度学习框架和生态）以及硬件（GPU）上的计算能力的突破，人工智能得以有可能以较低的成本去替代掉较高的人工成本，今年大热起来。

神经网络目前主要应用在图像（密集型矩阵），语音（密集型矩阵）和文本（稀疏型矩阵）分类识别。

# 二 基本概念和原理

## 2.1 网络结构

### 一个最简单的全连接神经网络

如下图是一个最普通的全连接的神经网络（全连接网络：每一层节点的输出结果会发送给下一层的所有节点），最左边的方形代表输入层，最右边的圆代表输出层，前面一层的节点会通过“边”把本节点的值传递给后面一层的所有节点。对于后面一层的节点来说，每个节点都接受前面一层所有节点的值。

![网络结构](http://blog.luojia.ren/images/神经网络/网络结构.png)

### 单个神经元

一个神经网络由 n 个神经元组成，每个神经元做的事情都如下图所示。一是接受上一层的输入通过特定的线性计算公式计算出本节点的值；二是通过激活函数来对前面已经计算得到的结果做一个非线性计算。

![神经元](http://blog.luojia.ren/images/神经网络/神经元.png)

> 为什么需要激活函数？我们通常认为神经网络中的一层是对数据的一次非线性映射。神经元所做的第一步实现了对输入数据的范围变换，空间旋转以及平移操作，而非线性的激活函数则完成了对输入数据的原始空间的扭曲。当网络层数变多的时候，在前面层学习的初步特征的基础上，后面层的网络可以形成更加高级的特征，对原始的空间的扭曲也更大。很多复杂的任务需要高度的非线性分界面，深度更深的网络可以比浅层的神经网络有更好的表达。


## 2.2 激活函数

主要作用提供规模化的非线性能力，模拟的是神经元的被激发的概念。

### 常用的激活函数

函数| Sigmoid | tanh | ReLu（普遍率最高）
--- | ---| ---| ---
形状 | ![Sigmoid](http://blog.luojia.ren/images/神经网络/sigmoid.png) | ![Sigmoid](http://blog.luojia.ren/images/神经网络/tanh.png) | ![Sigmoid](http://blog.luojia.ren/images/神经网络/relu.png)
优点 | 在任何区间都是可导的| 中心对称；任何区间可导 | 学习的效率高
缺点 | 不是中心对称；当值趋向较大或者较小的时候，学习的效率很低 | 当值趋向较大或者较小的时候，学习的效率很低 | 在小于 0 的区间段会有点问题
情景推荐 | | |

## 2.3 损失函数

用来评价学习出来的输出和我们预期的输出之间的差距。

### 常用损失函数

1. 0-1 损失函数：如果预测值和真实值一样，损失值就为 0；如果不一样，损失值就为1
2. 绝对值损失函数：利用两个数的差的绝对值作为衡量预测值和真实值之间的差
3. 均方误差损失函数：计算模型输出与真实输出的差的平方，再把 n 个样本的差平方加起来，然后求平均，得到均方差

## 2.4 梯度下降

通过一种渐进性的形式来调整我们整个函数的形态，目的是让损失函数趋近于全局最小值。


## 2.5 反向传播

通过每一次的训练结果逆向来调节每一层的每一个神经元的 w 和 b 的值来达到全局损失最小。

## 2.6 训练过程理解

> 传播按层进行，中间没有交叉，所有层全部算好后再一次性更新参数

![训练过程](http://blog.luojia.ren/images/神经网络/训练过程.png)